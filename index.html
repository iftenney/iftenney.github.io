<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ian  Tenney</title>
<meta name="description" content="Ian Tenney's personal website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RPS0P4NJ1V"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-RPS0P4NJ1V');
  </script>


    

  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  
  <a href="https://twitter.com/iftenney" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  <a href="http://sigmoid.social/@iftenney" rel="me" target="_blank" title="@iftenney@sigmoid.social"><i class="fab fa-mastodon"></i></a>
  
  
  <a href="https://scholar.google.com/citations?user=7WntHrAAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/iftenney" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/iftenney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  
  
  
  
  
  <a href="https://research.google/people/IanTenney" target="_blank" title="Google Research"><i class="fab fa-google"></i></a>
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
      
  <span class="first-name">Ian</span>  <span class="last-name">Tenney</span>


    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-left">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/iftenney.jpg">
      
      
    </div>
    

    <div class="clearfix main-text-content">
      <p>I am a Staff Research Scientist on the <a href="https://pair.withgoogle.com/">People + AI Research</a> (PAIR) team in Google Research. My group focuses on interpretability for large langauge models (LLMs), including visualization tools, attribution methods, and intrinsic analysis (a.k.a. <a href="https://twitter.com/tallinzen/status/1139514475147649026">BERTology</a>) of model representations.</p>

<p>I am a co-creator and TL of the <a href="https://pair-code.github.io/lit/">Learning Interpretability Tool <span class="nobreak">(<img src="/assets/img/fire_emoji.png" />LIT)</span></a>.</p>

<p>Previously, I’ve taught <a href="https://www.ischool.berkeley.edu/courses/datasci/266">an NLP course</a> at UC Berkeley School of Information. In a past life I was a physicist, studying ultrafast molecular and optical physics in the lab of <a href="https://profiles.stanford.edu/philip-bucksbaum">Philip H. Bucksbaum</a> at <a href="https://ultrafast.stanford.edu/">Stanford / SLAC</a>.</p>

<p>Contact: <code class="language-plaintext highlighter-rouge">"if" + lastname + "@gmail.com"</code> (or <code class="language-plaintext highlighter-rouge">@google.com</code>)</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Apr&nbsp;15,&nbsp;2024</th>
          <td>
            
              New preprint! <a href="https://arxiv.org/abs/2404.07498">Interactive Prompt Debugging with Sequence Salience</a> goes into more detail on the prompt debugging tool we previously released for Gemma. Sequence Salience now works for Mistral and Llama 2, and features a more in-depth tutorial at <a href="http://goo.gle/sequence-salience">goo.gle/sequence-salience</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar&nbsp;1,&nbsp;2024</th>
          <td>
            
              New preprint! <a href="https://arxiv.org/abs/2402.10524">LLM Comparator</a>, a visualization tool to help LLM developers make sense of side-by-side evaluations, accepted to CHI Late-breaking Work.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb&nbsp;21,&nbsp;2024</th>
          <td>
            
              <a href="https://github.com/PAIR-code/lit/releases/tag/v1.1"><img class="inline-image" src="/assets/img/fire_emoji.png" />LIT v1.1</a> featured in <a href="https://blog.google/technology/developers/gemma-open-models/">The Keyword</a> as the debugging tool for the new <a href="https://ai.google.dev/gemma/">Gemma</a> family of open models from Google.

As part of the <a href="https://ai.google.dev/responsible">Responsible Generative AI Toolkit</a>, use the new <a href="https://ai.google.dev/responsible/model_behavior">sequence salience</a> feature to debug complex LLM prompts, such as few-shot, chain-of-thought, or constitutions. Try it in Colab here: <a href="https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb">Using LIT to Analyze Gemma Models in Keras</a>

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    <div class="projects">
      <h2>projects</h2>
      <div class="projects">
  <!-- <div class='row'> -->
  <div class="card-deck">

  
  
    <div class="card hoverable">
      
      <a class="card-contents" href="/projects/viz/">
      
      
      <div class="image-container">
        <img src="/assets/img/lit-emb-projector-3.png" alt="project thumbnail" class="card-img-top">
      </div>
      
      <div class="card-body">
        <h2 class="card-title">Interpretability & Visualization Tools</h2>
        <p class="card-text">Visualize, inspect, and debug behavior of LLMs and other ML models.</p>
        <div class="row ml-1 mr-1 p-0">
          
        </div>
      </div>
      </a>
    </div>
  
    <div class="card hoverable">
      
      <a class="card-contents" href="/projects/tda/">
      
      
      <div class="image-container">
        <img src="/assets/img/fact-tracing-figure1.png" alt="project thumbnail" class="card-img-top">
      </div>
      
      <div class="card-body">
        <h2 class="card-title">Training-Data Attribution</h2>
        <p class="card-text">Understanding model behavior by finding training examples that influence a particular prediction.</p>
        <div class="row ml-1 mr-1 p-0">
          
        </div>
      </div>
      </a>
    </div>
  
    <div class="card hoverable">
      
      <a class="card-contents" href="/projects/bertology/">
      
      
      <div class="image-container">
        <img src="/assets/img/bertology.png" alt="project thumbnail" class="card-img-top">
      </div>
      
      <div class="card-body">
        <h2 class="card-title">Probing & BERTology</h2>
        <p class="card-text">Understanding linguistic structure in pre-trained language models, such as ELMo and BERT.</p>
        <div class="row ml-1 mr-1 p-0">
          
        </div>
      </div>
      </a>
    </div>
  

  </div>
  <!-- </div> -->
</div>

    </div>

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/seqsal-hero.png" alt="Interactive Prompt Debugging with Sequence Salience">
    
  </div>

  <div id="tenney2024interactive" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2404.07498.pdf" target="_blank"><div class="title">Interactive Prompt Debugging with Sequence Salience</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Ryan Mullins,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Bin Du,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Shree Pandya,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Minsuk Kahng,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Lucas Dixon
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint, </em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2404.07498" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://ai.google.dev/responsible/model_behavior" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
    
    
      <a href="https://goo.gle/sequence-salience" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present Sequence Salience, a visual tool for interactive prompt debugging with input salience methods. Sequence Salience builds on widely used salience methods for text classification and single-token prediction, and extends this to a system tailored for debugging complex LLM prompts. Our system is well-suited for long texts, and expands on previous work by 1) providing controllable aggregation of token-level salience to the word, sentence, or paragraph level, making salience over long inputs tractable; and 2) supporting rapid iteration where practitioners can act on salience results, refine prompts, and run salience on the new output. We include case studies showing how Sequence Salience can help practitioners work with several complex prompting strategies, including few-shot, chain-of-thought, and constitutional principles. Sequence Salience is built on the Learning Interpretability Tool, an open-source platform for ML model visualizations, and code, notebooks, and tutorials are available at http://goo.gle/sequence-salience.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/llm-comparator-figure1.png" alt="LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models">
    
  </div>

  <div id="kahng2024llm" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2402.10524.pdf" target="_blank"><div class="title">LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Minsuk Kahng,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Mahima Pushkarna,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Michael Xieyang Liu,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  James Wexler,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Emily Reif,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Krystal Kallarackal,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Minsuk Chang,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Michael Terry,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Lucas Dixon
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>CHI Late-Breaking Work, </em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2402.10524" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/simfluence-2a.png" alt="Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs">
    
  </div>

  <div id="guu2023simfluence" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2303.08114.pdf" target="_blank"><div class="title">Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Kelvin Guu,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Albert Webson,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Ellie Pavlick,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Lucas Dixon,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Tolga Bolukbasi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint, </em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2303.08114" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://twitter.com/kelvin_guu/status/1637835146874478592" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fab fa-twitter"></i>Thread</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Training data attribution (TDA) methods offer to trace a model’s prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects.
To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, “If my model had trained on example z1, then z2, ..., then zn, how would it behave on ztest?”; the simulator should then output a simulated training run, which is a time series predicting the loss on ztest at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur.
We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman’s correlation and reducing mean-squared error by 75%) across several tasks, models, and training methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/multiberts-figure3.png" alt="The MultiBERTs: BERT Reproductions for Robustness Analysis">
    
  </div>

  <div id="sellam2022multiberts" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2106.16163.pdf" target="_blank"><div class="title">The MultiBERTs: BERT Reproductions for Robustness Analysis</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Thibault Sellam,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Steve Yadlowsky,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jason Wei,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Naomi Saphra,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Alexander D’Amour,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Tal Linzen,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jasmijn Bastings,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Iulia Turc,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jacob Eisenstein,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Dipanjan Das,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ellie Pavlick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR (spotlight), </em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2106.16163" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      
      <a href="https://openreview.net/forum?id=K0E_F0gFDgA" class="btn btn-sm z-depth-0" role="button" target="_blank">ICLR 2022</a>
      
    
    
    
    
    
    
    
      <a href="https://github.com/google-research/language/tree/master/language/multiberts" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternative strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. The models and statistical library are available online, along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/lit-emb-projector-3.png" alt="The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models">
    
  </div>

  <div id="tenney2020lit" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2008.05122.pdf" target="_blank"><div class="title">The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  James Wexler,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jasmijn Bastings,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Tolga Bolukbasi,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Andy Coenen,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Sebastian Gehrmann,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Ellen Jiang,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Mahima Pushkarna,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Carey Radebaugh,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Emily Reif,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ann Yuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, </em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2008.05122" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.emnlp-demos.15" class="btn btn-sm z-depth-0" role="button" target="_blank">EMNLP 2020</a>
      
    
    
    
    
    
    
      <a href="https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/PAIR-code/lit" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
      
      <a href="/assets/pdf/lit-poster-emnlp-2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://pair-code.github.io/lit/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models—including classification, seq2seq, and structured prediction—and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/bert-by-layer.png" alt="BERT Rediscovers the Classical NLP Pipeline">
    
  </div>

  <div id="tenney2019bert" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/1905.05950.pdf" target="_blank"><div class="title">BERT Rediscovers the Classical NLP Pipeline</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Dipanjan Das,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ellie Pavlick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 57th Conference of the Association for Computational Linguistics, </em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1905.05950" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/P19-1452/" class="btn btn-sm z-depth-0" role="button" target="_blank">ACL 2019</a>
      
    
    
    
    
    
    
    
      <a href="https://github.com/nyu-mll/jiant-v1-legacy/tree/master/probing" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
      
      <a href="/assets/pdf/bert-layer-poster-acl-2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/edge-probing.png" alt="What do you learn from context? Probing for sentence structure in contextualized word representations">
    
  </div>

  <div id="tenney2019what" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/1905.06316.pdf" target="_blank"><div class="title">What do you learn from context? Probing for sentence structure in contextualized word representations</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Patrick Xia,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Berlin Chen,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Alex Wang,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Adam Poliak,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  R Thomas McCoy,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Najoung Kim,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Benjamin Van Durme,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Sam Bowman,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Dipanjan Das,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ellie Pavlick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, </em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1905.06316" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      
      <a href="https://openreview.net/forum?id=SJzSgnRcKX" class="btn btn-sm z-depth-0" role="button" target="_blank">ICLR 2019</a>
      
    
    
    
    
    
    
    
      <a href="https://github.com/nyu-mll/jiant-v1-legacy/tree/master/probing" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
      
      <a href="/assets/pdf/edgeprobe-poster-iclr-2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Ian  Tenney.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 15, 2024.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  

  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
