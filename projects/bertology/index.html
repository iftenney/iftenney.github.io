<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ian  Tenney | Probing & BERTology</title>
<meta name="description" content="Ian Tenney's personal website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/projects/bertology/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RPS0P4NJ1V"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-RPS0P4NJ1V');
  </script>


    

  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
        
  <span class="first-name">Ian</span>  <span class="last-name">Tenney</span>


      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Probing & BERTology</h1>
    <p class="post-description">Understanding linguistic structure in pre-trained language models, such as ELMo and BERT.</p>
  </header>

  <article>
    <p>Pre-trained language models have led to dramatic advancements in NLP capability in recent years, handily outperforming pipelined systems and non-contextual embeddings on most common tasks. What makes them so powerful? As a model like <a href="https://arxiv.org/abs/1810.04805">BERT</a> learns to fill in blanks or predict the next word, what kind of linguistic or world knowledge does it acquire? How is this information organized: does the model learn the same rules a human might, or develop it’s own idiosyncratic understanding? And: how is this information used, if such a model is asked to classify text, answer questions, or perform other downstream NLP tasks?</p>

<p>For an excellent primer on what we do - and don’t - know in this space, also see <a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00349">A Primer in BERTology: What We Know About How BERT Works</a> (Rogers et al. 2020).</p>

<div class="publications">
  <ol class="bibliography"><li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/counting-figure1.png" alt="Can Generative Multimodal Models Count to Ten?" />
    
  </div>

  <div id="rane2024can" class="col-sm-8">
    
      
        <a href="https://openreview.net/forum?id=ZdQfk4BN46" target="_blank"><div class="title">Can Generative Multimodal Models Count to Ten?</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Sunayana Rane,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Alexander Ku,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jason Michael Baldridge,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Thomas L. Griffiths,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Been Kim
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR 2024 Workshop on Representational Alignment, </em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      
      <a href="https://openreview.net/forum?id=ZdQfk4BN46" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>ICLR Workshops 2024</a>
      
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We adapt a developmental psychology paradigm to characterize the counting ability of the foundation model Parti. We show that three model scales of the Parti model (350m, 3B, and 20B parameters respectively) each have some counting ability, with a significant jump in performance between the 350m and 3B model scales. We also demonstrate that it is possible to interfere with these models’ counting ability simply by incorporating unusual descriptive adjectives for the objects being counted into the text prompt. We analyze our results in the context of the knower-level theory of child number learning. Our results show that we can gain experimental intuition for how to probe model behavior by drawing from a rich literature of behavioral experiments on humans, and, perhaps most importantly, by adapting human developmental benchmarking paradigms to AI models, we can characterize and understand their behavior with respect to our own.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/multiberts-figure3.png" alt="The MultiBERTs: BERT Reproductions for Robustness Analysis" />
    
  </div>

  <div id="sellam2022multiberts" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2106.16163.pdf" target="_blank"><div class="title">The MultiBERTs: BERT Reproductions for Robustness Analysis</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Thibault Sellam,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Steve Yadlowsky,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jason Wei,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Naomi Saphra,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Alexander D’Amour,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Tal Linzen,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jasmijn Bastings,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Iulia Turc,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jacob Eisenstein,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Dipanjan Das,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ellie Pavlick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICLR (spotlight), </em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2106.16163" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>arXiv</a>
    
    
      
      <a href="https://openreview.net/forum?id=K0E_F0gFDgA" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>ICLR 2022</a>
      
    
    
    
    
    
    
    
      <a href="https://github.com/google-research/language/tree/master/language/multiberts" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternative strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. The models and statistical library are available online, along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/lsl-figure1.png" alt="Asking without Telling: Exploring Latent Ontologies in Contextual Representations" />
    
  </div>

  <div id="michael2020asking" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2004.14513.pdf" target="_blank"><div class="title">Asking without Telling: Exploring Latent Ontologies in Contextual Representations</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Julian Michael,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Jan A. Botha,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                and <em>Ian Tenney</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), </em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2004.14513" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>arXiv</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.emnlp-main.552" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>EMNLP 2020</a>
      
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/scales-figure1.png" alt="Do Language Embeddings capture Scales?" />
    
  </div>

  <div id="zhang2020scales" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2010.05345.pdf" target="_blank"><div class="title">Do Language Embeddings capture Scales?</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Xikun Zhang,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Deepak Ramachandran,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Yanai Elazar,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Dan Roth
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: EMNLP, </em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2010.05345" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>arXiv</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.439" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>Findings of EMNLP 2020</a>
      
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance, and show that a simple method of canonicalizing numbers can have a significant effect on the results.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/finetune-figure2.png" alt="What Happens To BERT Embeddings During Fine-tuning?" />
    
  </div>

  <div id="merchant2020happens" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/2004.14448.pdf" target="_blank"><div class="title">What Happens To BERT Embeddings During Fine-tuning?</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                
                  Amil Merchant,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Elahe Rahimtoroghi,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Ellie Pavlick,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                and <em>Ian Tenney</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, </em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2004.14448" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>arXiv</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.4" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>BlackboxNLP 2020</a>
      
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques—supervised probing, unsupervised similarity analysis, and layer-based ablations—we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/bert-by-layer.png" alt="BERT Rediscovers the Classical NLP Pipeline" />
    
  </div>

  <div id="tenney2019bert" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/1905.05950.pdf" target="_blank"><div class="title">BERT Rediscovers the Classical NLP Pipeline</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Dipanjan Das,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ellie Pavlick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 57th Conference of the Association for Computational Linguistics, </em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1905.05950" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>arXiv</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/P19-1452/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>ACL 2019</a>
      
    
    
    
    
    
    
    
      <a href="https://github.com/nyu-mll/jiant-v1-legacy/tree/master/probing" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
    
    
      
      <a href="/assets/pdf/bert-layer-poster-acl-2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-chalkboard"></i>Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row bib-card">
  <div class="col-sm mt-3 mt-md-0">
    
      <img class="img-thumbnail" src="/assets/img/edge-probing.png" alt="What do you learn from context? Probing for sentence structure in contextualized word representations" />
    
  </div>

  <div id="tenney2019what" class="col-sm-8">
    
      
        <a href="https://arxiv.org/pdf/1905.06316.pdf" target="_blank"><div class="title">What do you learn from context? Probing for sentence structure in contextualized word representations</div></a>
      
      <div class="author">
        
          <!--  -->
          
          
          
          
            
              
                <em>Ian Tenney</em>,
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Patrick Xia,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Berlin Chen,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Alex Wang,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Adam Poliak,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  R Thomas McCoy,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Najoung Kim,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Benjamin Van Durme,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Sam Bowman,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  Dipanjan Das,
                
              
            
          
        
          <!--  -->
          
          
          
          
            
              
                
                  and Ellie Pavlick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, </em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1905.06316" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>arXiv</a>
    
    
      
      <a href="https://openreview.net/forum?id=SJzSgnRcKX" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file"></i>ICLR 2019</a>
      
    
    
    
    
    
    
    
      <a href="https://github.com/nyu-mll/jiant-v1-legacy/tree/master/probing" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i>Code</a>
    
    
    
    
      
      <a href="/assets/pdf/edgeprobe-poster-iclr-2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-chalkboard"></i>Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Ian  Tenney.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: June 01, 2024.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  

  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
